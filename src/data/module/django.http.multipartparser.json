{
  "functions": [
    {
      "name": "exhaust",
      "params": [
        {
          "prefix": "",
          "name": "stream_or_iterable",
          "annotation": null
        }
      ],
      "docstring": "\"\"\"Exhaust an iterator or stream.\"\"\""
    },
    {
      "name": "parse_boundary_stream",
      "params": [
        {
          "prefix": "",
          "name": "stream",
          "annotation": null
        },
        {
          "prefix": "",
          "name": "max_header_size",
          "annotation": null
        }
      ],
      "docstring": "\"\"\"\n    Parse one and exactly one stream that encapsulates a boundary.\n    \"\"\""
    },
    {
      "name": "parse_header",
      "params": [
        {
          "prefix": "",
          "name": "line",
          "annotation": null
        }
      ],
      "docstring": "\"\"\"\n    Parse the header into a key-value.\n\n    Input (line): bytes, output: str for key/name, bytes for values which\n    will be decoded later.\n    \"\"\""
    }
  ],
  "classes": [
    {
      "name": "MultiPartParserError",
      "methods": [],
      "classes": []
    },
    {
      "name": "InputStreamExhausted",
      "methods": [],
      "classes": [],
      "docstring": "\"\"\"\n    No more reads are allowed from this device.\n    \"\"\""
    },
    {
      "name": "MultiPartParser",
      "methods": [
        {
          "name": "parse",
          "params": [
            {
              "prefix": "",
              "name": "self",
              "annotation": null
            }
          ],
          "docstring": "\"\"\"\n        Parse the POST data and break it into a FILES MultiValueDict and a POST\n        MultiValueDict.\n\n        Return a tuple containing the POST and FILES dictionary, respectively.\n        \"\"\""
        },
        {
          "name": "handle_file_complete",
          "params": [
            {
              "prefix": "",
              "name": "self",
              "annotation": null
            },
            {
              "prefix": "",
              "name": "old_field_name",
              "annotation": null
            },
            {
              "prefix": "",
              "name": "counters",
              "annotation": null
            }
          ],
          "docstring": "\"\"\"\n        Handle all the signaling that takes place when a file is complete.\n        \"\"\""
        },
        {
          "name": "sanitize_file_name",
          "params": [
            {
              "prefix": "",
              "name": "self",
              "annotation": null
            },
            {
              "prefix": "",
              "name": "file_name",
              "annotation": null
            }
          ],
          "docstring": "\"\"\"\n        Sanitize the filename of an upload.\n\n        Remove all possible path separators, even though that might remove more\n        than actually required by the target system. Filenames that could\n        potentially cause problems (current/parent dir) are also discarded.\n\n        It should be noted that this function could still return a \"filepath\"\n        like \"C:some_file.txt\" which is handled later on by the storage layer.\n        So while this function does sanitize filenames to some extent, the\n        resulting filename should still be considered as untrusted user input.\n        \"\"\""
        }
      ],
      "classes": [],
      "docstring": "\"\"\"\n    A rfc2388 multipart/form-data parser.\n\n    ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks\n    and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.\n    \"\"\""
    },
    {
      "name": "LazyStream",
      "methods": [
        {
          "name": "tell",
          "params": [
            {
              "prefix": "",
              "name": "self",
              "annotation": null
            }
          ]
        },
        {
          "name": "read",
          "params": [
            {
              "prefix": "",
              "name": "self",
              "annotation": null
            },
            {
              "prefix": "",
              "name": "size",
              "annotation": null
            }
          ]
        },
        {
          "name": "close",
          "params": [
            {
              "prefix": "",
              "name": "self",
              "annotation": null
            }
          ],
          "docstring": "\"\"\"\n        Used to invalidate/disable this lazy stream.\n\n        Replace the producer with an empty list. Any leftover bytes that have\n        already been read will still be reported upon read() and/or next().\n        \"\"\""
        },
        {
          "name": "unget",
          "params": [
            {
              "prefix": "",
              "name": "self",
              "annotation": null
            },
            {
              "prefix": "",
              "name": "bytes",
              "annotation": null
            }
          ],
          "docstring": "\"\"\"\n        Place bytes back onto the front of the lazy stream.\n\n        Future calls to read() will return those bytes first. The\n        stream position and thus tell() will be rewound.\n        \"\"\""
        }
      ],
      "classes": [],
      "docstring": "\"\"\"\n    The LazyStream wrapper allows one to get and \"unget\" bytes from a stream.\n\n    Given a producer object (an iterator that yields bytestrings), the\n    LazyStream object will support iteration, reading, and keeping a \"look-back\"\n    variable in case you need to \"unget\" some bytes.\n    \"\"\""
    },
    {
      "name": "ChunkIter",
      "methods": [],
      "classes": [],
      "docstring": "\"\"\"\n    An iterable that will yield chunks of data. Given a file-like object as the\n    constructor, yield chunks of read operations from that object.\n    \"\"\""
    },
    {
      "name": "InterBoundaryIter",
      "methods": [],
      "classes": [],
      "docstring": "\"\"\"\n    A Producer that will iterate over boundaries.\n    \"\"\""
    },
    {
      "name": "BoundaryIter",
      "methods": [],
      "classes": [],
      "docstring": "\"\"\"\n    A Producer that is sensitive to boundaries.\n\n    Will happily yield bytes until a boundary is found. Will yield the bytes\n    before the boundary, throw away the boundary bytes themselves, and push the\n    post-boundary bytes back on the stream.\n\n    The future calls to next() after locating the boundary will raise a\n    StopIteration exception.\n    \"\"\""
    },
    {
      "name": "Parser",
      "methods": [],
      "classes": []
    }
  ],
  "docstring": "\"\"\"\nMulti-part parsing for file uploads.\n\nExposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to\nfile upload handlers for processing.\n\"\"\""
}